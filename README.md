Impacto Voice AI: Technical AssessmentAI Intern (LLMs & Voice AI)Candidate: SwastikProject: Voice-First Conversational AssistantPart A – System Design1. Architectural ComponentsThe system follows a modular, sequential pipeline designed for low latency and high reliability.Client-Side VAD (Voice Activity Detection): Implemented in the browser using Web Audio API (AnalyserNode). It monitors volume levels to detect silence (4-second threshold), reducing unnecessary server calls.STT Service (Speech-to-Text): Utilizes OpenAI’s Whisper-1 model. It handles multi-part form data (.wav) and transcribes binary audio into text.Session Context Manager: A stateful backend handler that maintains conversation history in a Python dictionary. It tracks a rolling window (last 10 messages) to provide memory without exceeding token limits.LLM Service: Powered by GPT-4o-mini. It uses a specialized system prompt to maintain a "concise voice assistant" persona, limited to 2-sentence responses for better Voice UX.TTS Service (Text-to-Speech): Uses OpenAI's TTS-1 (alloy voice) to convert text replies into high-quality MPEG audio.Frontend UI: A clean, CSS-gradient interface designed for mobile-first conversational flow.2. Data FlowInput: User speaks; Frontend captures audio and generates a session_id.Transmission: Audio blob + session_id are sent via POST to the FastAPI /api/voice endpoint.STT: Whisper transcribes audio $\rightarrow$ text.Inference: Text + Session History $\rightarrow$ GPT-4o-mini $\rightarrow$ Text Reply.Synthesis: Text Reply $\rightarrow$ TTS $\rightarrow$ Binary Audio.Output: Base64-encoded audio and text return to the Frontend for immediate playback and chat UI update.3. Critical Reasoning & Trade-offsLatency: To combat the "Thinking..." delay inherent in sequential STT-LLM-TTS pipelines, I chose the gpt-4o-mini model family, which significantly reduces Time-to-First-Byte (TTFB).Interruptibility (Barge-in): The frontend is programmed to immediately kill the AudioContext and current playback if the user taps the mic again, allowing for a natural conversation flow.Cost vs. Performance: Whisper-1 and GPT-4o-mini were selected to balance high-quality reasoning with low operational costs.Reliability: Implemented navigator.sendBeacon for session cleanup to ensure server resources are freed even if the user closes the tab abruptly.Part B – ImplementationCore TechnologiesBackend: FastAPI (Python 3.13), Uvicorn.AI Models: OpenAI Whisper (STT), GPT-4o-mini (LLM), TTS-1 (TTS).Frontend: Vanilla JavaScript, Web Audio API, HTML5/CSS3.Deployment: Render (Backend), Vercel (Frontend).Project StructurePlaintextbackend/
├── app/
│   ├── api/routes.py       # FastAPI Route definitions
│   ├── pipeline/assistant.py # Logic for STT->LLM->TTS flow
│   ├── stt/openai_stt.py   # Whisper Service
│   ├── llm/openai_llm.py   # GPT Service with History
│   ├── tts/openai_tts.py   # TTS Service
│   └── config.py           # Environment variables & Model Config
├── main.py                 # Entry point
└── requirements.txt        # Pinned dependencies
Installation & SetupClone the repository:Bashgit clone https://github.com/SwastiksGautam/Impacto-Voice-AI.git
cd backend
Install dependencies:Bashpip install -r requirements.txt
Environment Variables: Create a .env file in the backend/ folder:Code snippetOPENAI_API_KEY=your_sk_key_here
Run the server:Bashuvicorn app.main:app --reload
What We’re Evaluating (Self-Assessment)System Thinking: Demonstrated by the separation of STT, LLM, and TTS into distinct, interchangeable services.Voice Constraints: Solved the "long response" issue by enforcing 2-sentence limits in the system prompt.Error Handling: Implemented checks for transcription failures and backend connectivity issues.Scalability: While current sessions are in-memory, the architecture is "Redis-ready" for future production scaling.Next StepsI have fully deployed the Backend on Render and the Frontend on Vercel. You can test the live application here:Live App: https://impacto-voice-ai.vercel.appAPI Health Check: https://impacto-voice-ai.onrender.com/
